{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlc3zYOxxNM3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import psycopg2\n",
    "from urllib.request import urlretrieve\n",
    "from sqlalchemy import create_engine\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "INPUT_ROOT = pathlib.Path().cwd()/'source'\n",
    "WORKING_ROOT = pathlib.Path().cwd()/'working'\n",
    "OUTPUT_ROOT = pathlib.Path().cwd()/'output'\n",
    "MODELS_PATH = pathlib.Path().cwd()/'models'\n",
    "\n",
    "for path in [INPUT_ROOT, WORKING_ROOT, OUTPUT_ROOT, MODELS_PATH]:\n",
    "    path.mkdir(exist_ok=True)\n",
    "\n",
    "# Read config file\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Input files\n",
    "SOURCE_PATH = INPUT_ROOT/config[\"paths\"][\"source_path\"]\n",
    "TARGET_PATH = INPUT_ROOT/config[\"paths\"][\"target_path\"]\n",
    "\n",
    "# Size constants\n",
    "BATCH_SIZE = config[\"execution\"][\"batch_size\"]\n",
    "NUM_TOP_MATCHES = config[\"execution\"][\"num_top_matches\"]\n",
    "\n",
    "# Model links\n",
    "BIOWORDVEC_LINK=config[\"models\"][\"BioWordVec\"]\n",
    "SPACY_MODEL_LINK=config[\"models\"][\"Spacy_SciSM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USSiuW8KO5g1",
    "outputId": "f3e3167c-ff70-4cf5-f39a-466150e41ee7"
   },
   "outputs": [],
   "source": [
    "# Loading the biomedical language model\n",
    "model_name = SPACY_MODEL_LINK.split(\"/\")[-1]\n",
    "arch_model_path = MODELS_PATH/model_name\n",
    "if not arch_model_path.exists():\n",
    "    urlretrieve(SPACY_MODEL_LINK, arch_model_path)\n",
    "%pip install $arch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(model_name.split(\"-\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VVVO-NI3y6gy"
   },
   "outputs": [],
   "source": [
    "# Read the CSV files into dataframes\n",
    "voc1_df = pd.read_csv(SOURCE_PATH)\n",
    "voc2_df = pd.read_csv(TARGET_PATH)\n",
    "voc1_df=voc1_df[:10] # следующая 2000:4000\n",
    "voc1_df = voc1_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ly_W5-3gys4z",
    "outputId": "fe007f9f-e5be-4a21-b6a6-747d9ed77f41"
   },
   "outputs": [],
   "source": [
    "voc1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ipiPSe65zA7R",
    "outputId": "7d52bee6-0657-4a7d-bd44-f7d45a669426"
   },
   "outputs": [],
   "source": [
    "voc2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LE3JZo-u1B34"
   },
   "outputs": [],
   "source": [
    "# # words to remove from strings\n",
    "# stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\",\n",
    "#              'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "#              'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what',\n",
    "#              'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "#              'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "#              'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "#              'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
    "#              'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "#              'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other',\n",
    "#              'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "#              'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain',\n",
    "#              'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "#              \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',\n",
    "#              \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\",\n",
    "#              'wouldn', \"wouldn't\",\n",
    "#               # don't care  about right and left most of the time\n",
    "#              #'right','left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop_words that were received from SciSpacy model (UMLS based)\n",
    "stop_words = [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again',\n",
    "              'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', \n",
    "              'amongst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', \n",
    "              'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', \n",
    "              'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', \n",
    "              'between', 'beyond', 'both', 'bottom', 'but', 'by', 'ca', 'call', 'can', 'cannot', 'could', 'did', \n",
    "              'do', 'does', 'doing', 'done', 'down', 'due', 'during', 'each', 'eight', 'either', 'eleven', 'else', \n",
    "              'elsewhere', 'empty', 'enough', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', \n",
    "              'except', 'few', 'fifteen', 'fifty', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'four', \n",
    "              'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'have', 'he', 'hence', 'her', \n",
    "              'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', \n",
    "              'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', \n",
    "              'last', 'latter', 'latterly', 'least', 'less', 'made', 'make', 'many', 'may', 'me', 'meanwhile', \n",
    "              'might', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', \"n't\", \n",
    "              'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone',\n",
    "              'nor', 'not', 'nothing', 'now', 'nowhere', 'n‘t', 'n’t', 'of', 'off', 'often', 'on', 'once', 'one', \n",
    "              'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "              'part', 'per', 'perhaps', 'please', 'put', 'quite', 'rather', 're', 'really', 'regarding', 'same',\n",
    "              'say', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show',\n",
    "              'side', 'since', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime',\n",
    "              'sometimes', 'somewhere', 'still', 'such', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', \n",
    "              'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon',\n",
    "              'these', 'they', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', \n",
    "              'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'under', 'unless', \n",
    "              'until', 'up', 'upon', 'us', 'used', 'using', 'various', 'very', 'via', 'was', 'we', 'well', 'were', \n",
    "              'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "              'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole',\n",
    "              'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours',\n",
    "              'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m', '’re', '’s', '’ve']\n",
    "\n",
    "# aditional task-specific stop_words\n",
    "\n",
    "stop_words = stop_words + ['noc', 'nos', '[d]', 'unknown_unit', '|', 'see comment', 'due', 'nec', 'unspecified', '[v]', '(see comments)',\n",
    "              '(disorder)', '(procedure)', '(finding)', '(observable entity)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AZx21gOUGDC"
   },
   "outputs": [],
   "source": [
    "# Convert the titles to lowercase\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name'].str.lower()\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name'].str.lower()\n",
    "\n",
    "\n",
    "# Remove stopwords\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: ' '.join(filter(lambda word: word not in stop_words, x.split())))\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: ' '.join(filter(lambda word: word not in stop_words, x.split())))\n",
    "\n",
    "\n",
    "# ignores weird symbols\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x).encode('ascii', 'ignore').decode()) if isinstance(x, str) else str(x))\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x).encode('ascii', 'ignore').decode()) if isinstance(x, str) else str(x))\n",
    "\n",
    "# replace '-' with ' '\n",
    "#voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].map(lambda x: re.sub('-', ' ', x))\n",
    "#voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].map(lambda x: re.sub('-', ' ', x))\n",
    "\n",
    "# replace '/' with ' '\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].str.replace('/', ' ')\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].str.replace('/', ' ')\n",
    "\n",
    "\n",
    "# Remove punctuation, digits etc\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].str.replace(r'[^\\w\\s]','')\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].str.replace(r'[^\\w\\s]','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T40LH33YjtWa",
    "outputId": "3e892020-5817-4804-9dbb-4a53ae130def"
   },
   "outputs": [],
   "source": [
    "voc1_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "adGsTZn-FcIe",
    "outputId": "2c553d58-7eae-4f0c-ba8e-a3b71f8679aa"
   },
   "outputs": [],
   "source": [
    "voc2_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOc-mBIahXNN",
    "outputId": "34ecfefe-48b0-4963-9244-eb6cb77915d2"
   },
   "outputs": [],
   "source": [
    "def tokenize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "    return tokens\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(tokenize_with_spacy)\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(tokenize_with_spacy)\n",
    "\n",
    "elapsed_time = (time.time() - start_time)/60\n",
    "print(f\"Total elapsed time for tokenization: {elapsed_time:.1f} minutes\")\n",
    "\n",
    "\n",
    "def lemmatize_with_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: lemmatize_with_spacy(' '.join(x)))\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: lemmatize_with_spacy(' '.join(x)))\n",
    "\n",
    "elapsed_time = (time.time() - start_time)/60\n",
    "print(f\"Total elapsed time for lemmatization: {elapsed_time:.1f} minutes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "uQBEaOINlZBm",
    "outputId": "7b7221ab-7dac-41c4-ef6d-8d42f8a84c8f"
   },
   "outputs": [],
   "source": [
    "voc1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "c29UQE5hhBD0",
    "outputId": "79efcc8e-ee3c-433c-8c22-c1784390784c"
   },
   "outputs": [],
   "source": [
    "voc2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJBzfxcKlabA"
   },
   "outputs": [],
   "source": [
    "# Save previously prepared vocabulary for future use\n",
    "voc2_df.to_csv(WORKING_ROOT / (TARGET_PATH.stem + '_processed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and compute the TF-IDF document-term matrix for the snomed concepts\n",
    "voc2_docs = voc2_df['concept_name_processed'].apply(lambda x: ''.join(x)).tolist()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "voc2_dtm = tfidf_vectorizer.fit_transform(voc2_docs)\n",
    "\n",
    "# Calculate cosine similarities and find most similar snomed concepts for each meddra concept\n",
    "results = []\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "\n",
    "for voc1_index, voc1_row in voc1_df.iterrows():\n",
    "    voc1_tokens = voc1_row['concept_name_processed']\n",
    "    voc1_name = voc1_row['concept_name']\n",
    "    voc1_concept_id = voc1_row['concept_id']\n",
    "\n",
    "    voc1_text = ' '.join(voc1_tokens)\n",
    "    voc1_dtm = tfidf_vectorizer.transform([voc1_text])\n",
    "\n",
    "    similarities = cosine_similarity(voc1_dtm, voc2_dtm)[0]\n",
    "\n",
    "    # Find indices of most similar voc2 (SNOMED and etc.) concepts\n",
    "    top_indices = np.argsort(similarities)[-NUM_TOP_MATCHES:][::-1]\n",
    "\n",
    "    for max_index in top_indices:\n",
    "        voc2_concept_id = voc2_df.loc[max_index, 'concept_id']\n",
    "        voc2_name = voc2_df.loc[max_index, 'concept_name']\n",
    "        similarity = similarities[max_index]\n",
    "\n",
    "        # Add a separate line of information to the results\n",
    "        results.append((voc1_name, voc1_concept_id, voc2_concept_id, voc2_name, similarity))\n",
    "\n",
    "    processed_count += 1\n",
    "    if processed_count % BATCH_SIZE == 0:\n",
    "        elapsed_time = (time.time() - start_time) / 60\n",
    "        print(f\"Processed {processed_count} concepts in {elapsed_time:.1f} minutes\")\n",
    "\n",
    "# Sort the results by similarity in descending order\n",
    "results.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "columns = ['voc1_name', 'voc1_concept_id', 'voc2_concept_id', 'voc2_name', 'similarity']\n",
    "results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results as a Pandas dataframe\n",
    "df_sorted = results_df.sort_values(by=['voc1_name', 'similarity'], ascending=False)\n",
    "df_sorted['origin']='TfIDF'\n",
    "df_sorted.to_csv(WORKING_ROOT/('final_table_tfidf.csv'), index=False)\n",
    "df_sorted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf=df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzz_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rapidfuzz.fuzz.ratio** is a function from the rapidfuzz library in Python, which is used for string matching and similarity comparisons. The rapidfuzz library is known for its efficient and fast computation of string similarities, often utilized in scenarios where large datasets require rapid processing of string comparison tasks.\n",
    "\n",
    "The fuzz.ratio function computes the similarity between two strings, returning a score that represents how similar the two strings are. The score is an integer ranging from 0 to 100, where 100 indicates an exact match (the strings are identical), and lower scores indicate less similarity.\n",
    "\n",
    "This function implements an algorithm similar to the Levenshtein Distance, which calculates the number of single-character edits (insertions, deletions, or substitutions) required to change one word into the other. However, rapidfuzz has optimized the computation for performance, making it faster than many other implementations of similar algorithms.\n",
    "\n",
    "In summary, rapidfuzz.fuzz.ratio is a fast and efficient way to compare two strings and quantify their similarity, widely used in tasks like data cleaning, deduplication, and matching in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc1_df['concept_name_processed'] = voc1_df['concept_name_processed'].apply(lambda x: sorted(x))\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(', '))\n",
    "voc2_df['concept_name_processed'] = voc2_df['concept_name_processed'].apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc2_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Old variant of Levenshtein\n",
    "# start_time = time.time()\n",
    "# results = []\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for voc1_index, voc1_row in voc1_df.iterrows():\n",
    "#     voc1_name = voc1_row['concept_name']\n",
    "#     voc1_concept_id=voc1_row['concept_id']\n",
    "#     voc1_tokens = voc1_row['concept_name_processed']\n",
    "\n",
    "#     min_distances = [float('inf')] * NUM_TOP_MATCHES  # Список для хранения наименьших расстояний\n",
    "#     top_matches = [[] for _ in range(NUM_TOP_MATCHES)]  # Список для хранения ближайших сочетаний\n",
    "\n",
    "#     for voc2_index, voc2_row in voc2_df.iterrows():\n",
    "#         voc2_concept_id = voc2_row['concept_id']\n",
    "#         voc2_name = voc2_row['concept_name_processed']\n",
    "\n",
    "#         # Calculate Levenshtein distance between voc1 and voc2 concept names\n",
    "#         distance = lev.distance(''.join(voc1_tokens), ''.join(voc2_name))\n",
    "\n",
    "#         # Обновляем ближайшие совпадения, если найдено более близкое совпадение\n",
    "#         for i in range(NUM_TOP_MATCHES):\n",
    "#             if distance < min_distances[i]:\n",
    "#                 min_distances.insert(i, distance)\n",
    "#                 min_distances.pop()\n",
    "#                 top_matches.insert(i, (voc2_concept_id, voc2_row['concept_name'], distance))\n",
    "#                 top_matches.pop()\n",
    "#                 break\n",
    "\n",
    "#     # Сортируем top_matches по возрастанию min_distance перед добавлением в results\n",
    "#     top_matches.sort(key=lambda x: x[2])\n",
    "\n",
    "#     # Добавляем ближайшие совпадения в результаты\n",
    "#     for matches in top_matches:\n",
    "#         if matches:  # Проверяем, что список не пустой\n",
    "#             voc2_concept_id, voc2_name, min_distance = matches\n",
    "#             results.append((voc1_name, voc1_concept_id, voc2_concept_id, voc2_name, min_distance))\n",
    "\n",
    "#     count += 1\n",
    "#     if count % 10 == 0:\n",
    "#         elapsed_time = (time.time() - start_time) / 60\n",
    "#         print(f\"Elapsed time: {elapsed_time:.1f} minutes for {count} концептов\")\n",
    "\n",
    "# # Convert results to DataFrame\n",
    "# columns = ['voc1_name', 'voc1_concept_id', 'voc2_concept_id', 'voc2_name', 'min_distance']\n",
    "# results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output the results as a Pandas dataframe\n",
    "# df_sorted = results_df.sort_values(by=['voc1_name', 'min_distance'], ascending=True)\n",
    "# df_sorted['origin']='Levenshtein_distance'\n",
    "# df_sorted.to_csv('final_table'+'_levenshtein_old.csv', index=False)\n",
    "# df_sorted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version from ChatGPT\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process\n",
    "import concurrent.futures\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Preprocessing\n",
    "voc1_df['voc1_tokens_str'] = voc1_df['concept_name_processed'].apply(''.join)\n",
    "voc2_names = voc2_df['concept_name_processed'].apply(''.join).tolist()\n",
    "\n",
    "# Define a function to process a chunk of rows\n",
    "def process_chunk(chunk, voc2_names):\n",
    "    results = []\n",
    "    for _, voc1_row in chunk.iterrows():\n",
    "        voc1_name = voc1_row['concept_name']\n",
    "        voc1_concept_id = voc1_row['concept_id']\n",
    "        voc1_tokens_str = voc1_row['voc1_tokens_str']\n",
    "\n",
    "        # Get top matches using rapidfuzz\n",
    "        top_matches = process.extract(voc1_tokens_str, voc2_names, limit=NUM_TOP_MATCHES, scorer=rapidfuzz.fuzz.ratio)\n",
    "\n",
    "        # Unpack the results correctly\n",
    "        for match in top_matches:\n",
    "            matched_string, score, index = match\n",
    "            voc2_concept_id = voc2_df.iloc[index]['concept_id']\n",
    "            voc2_name = voc2_df.iloc[index]['concept_name']\n",
    "            results.append((voc1_name, voc1_concept_id, voc2_concept_id, voc2_name, score))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Split DataFrame into chunks for parallel processing\n",
    "chunks = [voc1_df.iloc[i:i + BATCH_SIZE] for i in range(0, voc1_df.shape[0], BATCH_SIZE)]\n",
    "\n",
    "# Parallel processing of chunks\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_chunk, chunks, [voc2_names] * len(chunks)))\n",
    "\n",
    "# Flatten the list of results\n",
    "flattened_results = [item for sublist in results for item in sublist]\n",
    "\n",
    "# Convert results to DataFrame\n",
    "columns = ['voc1_name', 'voc1_concept_id', 'voc2_concept_id', 'voc2_name', 'fuzz_ration']\n",
    "results_df = pd.DataFrame(flattened_results, columns=columns)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results as a Pandas dataframe\n",
    "df_sorted = results_df.sort_values(by=['voc1_name', 'fuzz_ratio'], ascending=False)\n",
    "df_sorted['origin']='fuzz_ratio'\n",
    "df_sorted.to_csv(WORKING_ROOT / 'final_table_fuzz_ratio.csv', index=False)\n",
    "df_sorted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzz_ratio=df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BioWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the processed_name contains a list object\n",
    "# We need a sorted list\n",
    "for voc_df in (voc1_df, voc2_df):\n",
    "    if isinstance(voc_df.loc[voc_df.index[0], 'concept_name_processed'], list):\n",
    "        voc_df['concept_name_processed'] = voc_df['concept_name_processed'].apply(sorted)\n",
    "    else:\n",
    "        voc_df['concept_name_processed'] = voc_df['concept_name_processed'].apply(lambda x: sorted(x.strip(\"[]\").replace(\"'\", \"\").split(', ')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained word embeddings model\n",
    "model_path = MODELS_PATH / 'BioWordVec_PubMed_MIMICIII_d200.vec.bin'\n",
    "if not model_path.exists():\n",
    "    print(f\"Model is not found in {str(model_path)}. Downloading...\")\n",
    "    urlretrieve(BIOWORDVEC_LINK, model_path)\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "start_time = time.time()\n",
    "\n",
    "# Функция для векторизации текста с использованием BioWordVec\n",
    "def vectorize_text(text, model):\n",
    "    tokens = text.split()\n",
    "    vectors = [model[token] for token in tokens if token in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Усреднение векторов\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Возвращает нулевой вектор, если нет известных слов\n",
    "\n",
    "# Применение функции к медицинским терминам в обоих наборах данных\n",
    "voc1_df['voc1_vector'] = voc1_df['concept_name_processed'].apply(lambda x: vectorize_text(' '.join(x), word2vec_model))\n",
    "voc2_df['voc2_vector'] = voc2_df['concept_name_processed'].apply(lambda x: vectorize_text(' '.join(x), word2vec_model))\n",
    "\n",
    "# Удаление строк с отсутствующими векторами\n",
    "voc1_df = voc1_df.dropna(subset=['voc1_vector'])\n",
    "voc2_df = voc2_df.dropna(subset=['voc2_vector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "v1 = np.vstack(voc1_df['voc1_vector'])\n",
    "v2 = np.vstack(voc2_df['voc2_vector'])\n",
    "similarity = cosine_similarity(v1, v2)\n",
    "tops = (-similarity).argsort()[:, :NUM_TOP_MATCHES]\n",
    "\n",
    "results_df = pd.concat(     # concat (axis = 0) the list of data frames\n",
    "    [pd.concat( # generate a list of data frames by concatenating (axis = 1) selected rows and cols from voc1 and voc2\n",
    "        [voc1_df.loc[[i], ['concept_name', 'concept_id']].reset_index(drop=True),  # for each target concept from voc_1\n",
    "         voc2_df.loc[[t], ['concept_id', 'concept_name']].reset_index(drop=True),        # add potential mapping match from voc_2\n",
    "         pd.Series(similarity[i][t], name='Similarity')], axis= 1)                       # and similarity value\n",
    "     for i in range(0, len(voc1_df)) for t in tops[i]]                                   # `i` - target vocabulary index, `t` - index in top similarity array\n",
    ").reset_index(drop=True)\n",
    "\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old working but very slow code\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for voc1_index, voc1_row in voc1_df.iterrows():\n",
    "#     voc1_name = voc1_row['concept_name']\n",
    "#     voc1_vector = voc1_row['voc1_vector']\n",
    "#     voc1_concept_id = voc1_row['concept_id']\n",
    "\n",
    "#     similarities = []\n",
    "#     for voc2_index, voc2_row in voc2_df.iterrows():\n",
    "#         voc2_name = voc2_row['concept_name']  # Сохраняем concept_id\n",
    "#         voc2_concept_id = voc2_row['concept_id']\n",
    "#         voc2_vector = voc2_row['voc2_vector']\n",
    "\n",
    "#         if voc1_vector is not None and voc2_vector is not None:\n",
    "#             similarity = cosine_similarity([voc1_vector], [voc2_vector])[0][0]\n",
    "#             similarities.append((voc2_name, voc2_concept_id, similarity))\n",
    "\n",
    "#     similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "#     top_matches = similarities[:NUM_TOP_MATCHES]\n",
    "\n",
    "#     for voc2_name, voc2_concept_id, similarity in top_matches:\n",
    "#         results.append((voc1_name, voc1_concept_id, voc2_concept_id, voc2_name, similarity))  # Добавляем voc1_index и voc2_name\n",
    "\n",
    "#     # Счетчик каждых 10 обработанных строк\n",
    "#     if (voc1_index + 1) % 10 == 0:\n",
    "#         elapsed_time = (time.time() - start_time) / 60\n",
    "#         print(f\"Elapsed time: {elapsed_time:.1f} minutes for processed {voc1_index + 1} rows\")\n",
    "\n",
    "        \n",
    "# # Создать DataFrame с результатами\n",
    "# columns = ['voc1_name', 'voc1_concept_id', 'voc2_concept_id', 'voc2_name', 'similarity']  # Добавляем 'voc1_index'\n",
    "# results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# elapsed_time = (time.time() - start_time) / 60\n",
    "# print(f\"Total elapsed time: {elapsed_time:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification to new variant\n",
    "new_columns = list(results_df.columns)\n",
    "new_columns[0] = 'voc1_name'\n",
    "new_columns[1]='voc1_concept_id'\n",
    "new_columns[2]='voc2_concept_id'\n",
    "new_columns[3]='voc2_name'\n",
    "new_columns[4]='similarity'\n",
    "results_df.columns = new_columns\n",
    "\n",
    "\n",
    "# Присвойте новый список названий столбцов атрибуту 'columns' DataFrame\n",
    "#results_df['voc1_concept_id']=0\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results as a Pandas dataframe\n",
    "df_sorted = results_df.sort_values(by=['voc1_name', 'similarity'], ascending=False)\n",
    "df_sorted['origin']='BioWordVec'\n",
    "df_sorted.to_csv(WORKING_ROOT/'final_table_biowordvec.csv', index=False)\n",
    "df_sorted.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biowordvec=df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final merge tables and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_date = current_datetime.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all results\n",
    "loaded = []\n",
    "for table in WORKING_ROOT.glob(\"final_*.csv\"):\n",
    "    print(f\"Processing {table.name}...\")\n",
    "    df = pd.read_csv(table)\n",
    "    df.drop_duplicates(subset=['voc1_name', 'voc2_concept_id'], inplace=True)\n",
    "    loaded.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединить таблицы, устранить дублирующие строки с одинаковыми парами сорс-код и potential_concept_id\n",
    "table_name = 'combined_table_'+str(current_date)+'.csv'\n",
    "df_exp = pd.concat(loaded)\n",
    "df_exp = df_exp.sort_values(by=['voc1_name']) \n",
    "df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Группировка и создание поля flag\n",
    "grouped = df_exp.groupby(['voc1_name', 'voc2_concept_id'])\n",
    "\n",
    "def combine_origin(group):\n",
    "    if len(group) > 1:\n",
    "        group['flag'] = ' / '.join(sorted(group['origin']))\n",
    "        return group.iloc[:1]  # оставляем только одну строку для объединенных\n",
    "    else:\n",
    "        group['flag'] = group['origin'].iloc[0]\n",
    "        return group\n",
    "\n",
    "df_exp = grouped.apply(combine_origin).reset_index(drop=True)\n",
    "df_exp.drop(['origin'], axis=1, inplace=True)\n",
    "\n",
    "# Вывод результата\n",
    "df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df_exp.groupby(['flag']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = df_exp.drop_duplicates(subset=['voc1_name', 'voc2_concept_id'])\n",
    "df_exp.to_excel('my_table_30112023.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp = pd.read_excel('my_table_30112023.xlsx')\n",
    "df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To download table into database scheme \n",
    "login=\"dev_test5\"\n",
    "passw = \"7hGg365$%fhhTfr$dJ\"\n",
    "engine = create_engine('postgresql://{}:{}@ovh07.odysseusinc.com:5555/postgres'.format(login, passw))\n",
    "table_name = 'am_gpt_meddra_pt_first_2000_051223_out_75_full_variant'\n",
    "df_exp.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "print(f'Import {table_name} was sucessful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = df_exp.copy()\n",
    "combined_df['question'] = 'Pick the best semantic match for '\n",
    "combined_df = combined_df[['question'] + [col for col in combined_df.columns if col != 'question']]\n",
    "combined_df.rename(columns={'voc1_name': 'source_code_description', 'voc2_concept_id': 'potential_target_concept_id'}, inplace=True)\n",
    "combined_df = combined_df.sort_values(by=['source_code_description', 'potential_target_concept_id'])\n",
    "combined_df = combined_df[['question', 'source_code_description', 'potential_target_concept_id']]\n",
    "combined_df['chatgptreply']=''\n",
    "combined_df['target_concept_id'] = None\n",
    "combined_df['log_id']=None\n",
    "combined_df['target_concept_id'] = combined_df['target_concept_id'].astype('Int64')\n",
    "combined_df['log_id'] = combined_df['log_id'].astype('Int64')\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To download table into database scheme \n",
    "login=\"dev_test5\"\n",
    "passw = \"7hGg365$%fhhTfr$dJ\"\n",
    "engine = create_engine('postgresql://{}:{}@ovh07.odysseusinc.com:5555/postgres'.format(login, passw))\n",
    "table_name = 'am_gpt_meddra_pt_first_2000_051223_out_75'\n",
    "combined_df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "print(f'Import {table_name} was sucessful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
